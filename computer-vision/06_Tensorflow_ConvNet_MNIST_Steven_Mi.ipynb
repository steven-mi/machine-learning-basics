{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "06_Tensorflow_ConvNet_MNIST_Steven-Mi.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXlk954chnX6"
      },
      "source": [
        "<h1>Convolutional Neural Networks anhand von MNIST</h1>\n",
        "\n",
        "<p>Das neuronale Netzwerk der letzten &Uuml;bung brauchte nur einige hundert Megabyte Arbeitsspeicher. Die 60000 MNIST Bilder mit ihren 784 Pixeln, die Gewichtsmatrizen und alle Zwischenergebnisse, die im Netzwerk beim Vorw&auml;rtspass berechnet wurden, sind keine 350MB gro&szlig;:<br />\n",
        "(60000&lowast; 784 + 784&lowast; 300 + 60000 &lowast; 300 + 300 &lowast; 10 + 60000 &lowast; 10) &lowast; 4 = 335MB <br />(MNISTData + Weights1 + Intermediate + Weight2 + Predictions) &lowast; Float32</p>\n",
        "\n",
        "<p>Dieser Verbrauch steigt rasant an, wenn Konvolutionsfilter ins Spiel kommen. Werden die Eingangsdaten mit 10 Filtern beliebiger Gr&ouml;&szlig;e (z.b. 3x3) gefalten, sind die ausgehenden Daten 10 mal so gro&szlig;:<br />\n",
        "(60000&lowast;784+10&lowast;3&lowast;3+60000&lowast;10&lowast;784)&lowast;4=2GB</p>\n",
        "\n",
        "<p>Um das zu verhindern, sollten in Zukunft nicht mehr alle Daten auf einmal im Netzwerk verarbeitet werden. Stattdessen werden Mini-Batches ben&ouml;tigt.</p>\n",
        "\n",
        "<p>Das komplette Notebook steht wieder zum <a href=\"06_Tensorflow_ConvNet_MNIST_Vorlage.ipynb\">download</a> bereit.</p>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<h2>Neural Networks mit Mini-Batches</h2>\n",
        "\n",
        "<p>Im folgenden ist ein zweischichtiges neuronales Netzwerk implementiert, welches alle MNIST Ziffern auf einmal verarbeitet. Bauen Sie den Code so um, dass er stattdessen mit Mini-Batches funktioniert. Dabei&nbsp;k&ouml;nnen Sie Numpy verwenden und die Daten als Mini-Batch in den Computation-Graph von Tensorflow geben&nbsp;oder Sie benutzen Tensorflows Batch-Methoden, um die Batches innerhalb eines Graphens zu erzeugen. Die K&ouml;nigsdiziplin sind Tensorflow Esitmators, die die Arbeit des Batchings &uuml;bernehmen, aber viele andere Anforderungen an das Netzwerk haben.&nbsp;Wichtig ist in allen&nbsp;F&auml;llen, dass auch die Testdaten gebatched werden.</p>\n",
        "\n",
        "<ul>\n",
        "\t<li><strong>Numpy</strong>: Es ist m&ouml;glich die Daten einmalig in kleine Batches zu unterteilen und diese dann in zuf&auml;lliger Reihnfolge in den Computation-Graph zu geben. Besser jedoch ist die Variante, bei der erst im letzten Moment ein Batch aus dem gesamten Datensatz extrahiert wird. Der Extraktionsbereich sollte dabei zuf&auml;llig gew&auml;hlt sein.&nbsp;&nbsp;</li>\n",
        "\t<li><strong>Tensorflow <a href=\"https://www.tensorflow.org/guide/datasets\" target=\"_blank\">Dataset</a></strong>: Sind die&nbsp;Daten klein genug, dass Sie&nbsp;in den Arbeitsspeicher, aber nicht mit einen Durchlauf durch das Netzwerk passen,  k&ouml;nnen sie zun&auml;chst&nbsp;komplett in den Graphen geladen werden und von dort in <a href=\"https://www.tensorflow.org/guide/datasets#batching_dataset_elements\" target=\"_blank\">kleine Batches</a> zerlegt werden. Mithilfe von <a href=\"https://www.tensorflow.org/guide/datasets#creating_an_iterator\"  target=\"_blank\">Iteratoren</a> können diese Batches dann einzeln in das Netzwerk geschickt werden.</li> \n",
        "<li><strong>Tensorflow Estimator</strong>: Innerhalb der High-Level API von Tensorflow gibt es die Möglichkeit, Estimators zu verwenden. Diese übernehmen sämtliche Batching-Arbeiten, verlangen aber bestimmte Eigenschaften vom Computation-Graphen. So m&uuml;ssen Trainings- und Evaluierungsmethoden in einen sogenannten <a href=\"https://www.tensorflow.org/api_docs/python/tf/estimator/EstimatorSpec\">EstimatorSpec</a> beschrieben werden, um sp&auml;ter mit einen <a href=\"http://www.tensorflow.org/api_docs/python/tf/estimator\">Estimator Model</a> arbeiten zu k&ouml;nnen.</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2Z4xzBKiGUt"
      },
      "source": [
        "!pip install tensorflow-gpu==1.15.0\n",
        "!pip install deep-teaching-commons"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-12-19T10:10:22.123731Z",
          "start_time": "2018-12-19T10:10:16.814055Z"
        },
        "id": "tf8aHW8YhnX9"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from shutil import copyfileobj\n",
        "from sklearn.datasets.base import get_data_home\n",
        "from deep_teaching_commons.data.fundamentals import mnist\n",
        "from sklearn.utils import check_random_state\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-12-19T10:10:36.160713Z",
          "start_time": "2018-12-19T10:10:24.000619Z"
        },
        "scrolled": true,
        "id": "-R_fu0-_hnYJ"
      },
      "source": [
        "X_train, y_train, X_test, y_test = mnist.Mnist().get_all_data(normalized=True, flatten=False)\n",
        "\n",
        "X_train = X_train.reshape((-1, 28, 28, 1))\n",
        "X_test = X_test.reshape((-1, 28, 28, 1))\n",
        "\n",
        "# only shuffle train dataset\n",
        "random_state = check_random_state(0)\n",
        "permutation = random_state.permutation(X_train.shape[0])\n",
        "X_train = X_train[permutation]\n",
        "y_train = y_train[permutation]\n",
        "\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)\n",
        "\n",
        "enc = OneHotEncoder()\n",
        "\n",
        "y_train = enc.fit_transform(np.expand_dims(y_train, axis=1)).toarray()\n",
        "y_test = enc.fit_transform(np.expand_dims(y_test, axis=1)).toarray()\n",
        "print(y_train.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sr4cPY8fhnYS"
      },
      "source": [
        "def minibatcher(inputs, targets, batchsize, shuffle=True):\n",
        "    assert len(inputs) == len(targets)\n",
        "    if shuffle:\n",
        "        indices = np.arange(len(inputs))\n",
        "        np.random.shuffle(indices)\n",
        "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
        "        if shuffle:\n",
        "            excerpt = indices[start_idx:start_idx + batchsize]\n",
        "        else:\n",
        "            excerpt = slice(start_idx, start_idx + batchsize)\n",
        "    yield inputs[excerpt], targets[excerpt]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAJpMQ1shnYY"
      },
      "source": [
        "<hr>\n",
        "\n",
        "<h2>Convolutional Neural Network mit MNIST Ziffern</h2>\n",
        "\n",
        "<p>Nachdem das neuronale Netzwerk mit Mini-Batches arbeitet, k&ouml;nnen die Fully-Connected (Dense) Layer mit Konvolutionsschichten ersetzt werden. Sinnvoll sind z.B. zwei Schichten mit 64 5x5 und 96 3x3 Filterkerneln. Um die Dimensionalit&auml;t der Daten langsam zu reduzieren, k&ouml;nnen entweder Schrittweiten bei den Konvolutionsschichten eingestellt werden oder Pooling angewendet werden. Zum Schluss ist es hilfreich, die hochdimensionalen Daten zu flatten, um sie in Dense Layern auf 10 Dimensionen herunterzubrechen. Berechnen Sie wieder den Trainingsfehler und die Testgenauigkeit. Zu erwarten sind Genauigkeiten von bis zu 99%.&nbsp;</p>\n",
        "\n",
        "<p>Je nachdem welche Tensorflow Version Sie nutzen (mindestens aber Version &gt;= 1.0), sind folgende Methoden hilfreich:</p>\n",
        "\n",
        "<ul>\n",
        "\t<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/nn/max_pool\" target=\"_blank\">tf.nn.max_pool</a> oder <a href=\"https://www.tensorflow.org/versions/master/api_docs/python/tf/layers/max_pooling2d\" target=\"_blank\">tf.layers.max_pooling2d</a></li>\n",
        "\t<li><a href=\"https://www.tensorflow.org/versions/master/api_docs/python/tf/nn/conv2d\" target=\"_blank\">tf.nn.conv2d</a> oder <a href=\"https://www.tensorflow.org/versions/master/api_docs/python/tf/layers/conv2d\" target=\"_blank\">tf.layers.conv2d</a></li>\n",
        "\t<li><a href=\"https://www.tensorflow.org/versions/master/api_docs/python/tf/reshape\" target=\"_blank\">tf.reshape</a>  oder <a href=\"https://www.tensorflow.org/versions/master/api_docs/python/tf/layers/flatten\" target=\"_blank\">tf.layers.Flatten</a></li>\n",
        "</ul>\n",
        "\n",
        "<p><strong>Optional</strong>: Yann LeCun hat vor fast 20 Jahren das MNIST Datenset herausgebracht und die Convolutionsnetzwerke erfunden. Damals gab es nicht die nötige Rechenleistung um in kurzer Zeit die notwendigen Filterkernel mittels Backpropagation und Gradient Descent zu erlernen. Seine Netzwerke sind daher sehr minimalistisch. Implementieren Sie das <a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf\" target=\"_blank\">LeNet5</a> Netzwerk nach seinen Vorbild. Padden Sie dazu die&nbsp;Eingangsdaten, damit die Bilder 32x32 Pixel haben und verwenden Sie nur 6, 16 und 120 Filterkernel&nbsp;(je 5x5 Pixel gro&szlig;) f&uuml;r die drei Konvolutionsschichten in LeNet5. Natürlich können Sie auch LeCun's <a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf\" target=\"_blank\">Stochastic gradient descent</a> nutzen um ihr Netzwerk zu trainieren oder gar den <a href=\"https://arxiv.org/pdf/1412.6980v8.pdf\" target=\"_blank\">Adam Optimizer</a>.</p>\n",
        "\n",
        "<p>&nbsp;</p>\n",
        "\n",
        "![LeNet5.png](attachment:LeNet5.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-12-19T10:10:41.111416Z",
          "start_time": "2018-12-19T10:10:40.702436Z"
        },
        "id": "OdeQA_3hhnYZ"
      },
      "source": [
        "# pixel count\n",
        "num_input = 28\n",
        "\n",
        "# num of classes\n",
        "num_classes = 10\n",
        "\n",
        "# learn rate\n",
        "learning_rate = 0.001\n",
        "\n",
        "batch_size = 128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-12-19T10:10:41.111416Z",
          "start_time": "2018-12-19T10:10:40.702436Z"
        },
        "id": "YhOJw9j3hnYf"
      },
      "source": [
        "# computation graph\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "    \n",
        "    # input data with fix shape to infer shapes of other graph nodes a build time\n",
        "    x_input = tf.placeholder(dtype=tf.float32, shape=[None, num_input, num_input, 1], name='x')\n",
        "    y_input = tf.placeholder(tf.int64, shape=[None, num_classes], name='y')\n",
        "        \n",
        "    layer1 = tf.layers.conv2d(inputs=x_input, filters=64, kernel_size=(5,5), activation=tf.nn.relu)\n",
        "    layer2 = tf.layers.conv2d(inputs=layer1, filters=64, kernel_size=(5,5), activation=tf.nn.relu)\n",
        "    flatten = tf.layers.flatten(layer2)\n",
        "    dense = tf.layers.dense(inputs=flatten, units=128, activation=tf.nn.relu)\n",
        "    prediction = tf.layers.dense(inputs=dense, units=num_classes)\n",
        "    \n",
        "    # compute trainings error\n",
        "    cost = tf.losses.softmax_cross_entropy(onehot_labels=y_input, logits=prediction)\n",
        "    \n",
        "    # use the Adam optimizer to derive the cost function and update the weights\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
        "\n",
        "    # accuracy for multiple batches\n",
        "    softmax = tf.nn.softmax(prediction)\n",
        "    acc, update_acc = tf.metrics.accuracy(labels=tf.argmax(y_input, 1), predictions=tf.argmax(softmax, axis=-1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-12-19T10:10:55.478117Z",
          "start_time": "2018-12-19T10:10:47.024838Z"
        },
        "id": "dMkB6t8ThnYi"
      },
      "source": [
        "# start a new session\n",
        "with tf.Session(graph=graph) as session:  \n",
        "    # initialize weights and bias variables\n",
        "    session.run(tf.group(tf.global_variables_initializer(), tf.local_variables_initializer()))     \n",
        "\n",
        "    # check against test set\n",
        "    print(\"Test accuracy \", session.run(update_acc, feed_dict={x_input: X_test, y_input: y_test}))\n",
        "\n",
        "    # reset accuracy\n",
        "    session.run(tf.local_variables_initializer())   \n",
        "\n",
        "    # train for a few iterations\n",
        "    ts = time.time()\n",
        "    train_errors = []\n",
        "    for i in range(100):\n",
        "        train_batcher = minibatcher(X_train, y_train, batch_size)\n",
        "        temp = []\n",
        "        for i, (X_batch, y_batch) in enumerate(train_batcher):\n",
        "            c, _ = session.run([cost, optimizer], feed_dict={x_input: X_batch, y_input: y_batch})\n",
        "            temp.append(c)\n",
        "        train_errors.append(np.mean(temp))\n",
        "    print(\"Improved train error from \", train_errors[0], \" to \", train_errors[-1], \" in \", str(time.time()-ts), \"secs\")\n",
        "\n",
        "    test_batcher = minibatcher(X_test, y_test, batch_size)\n",
        "    for i, (X_batch, y_batch) in enumerate(test_batcher):\n",
        "         session.run(update_acc, feed_dict={x_input: X_batch, y_input: y_batch})\n",
        "\n",
        "    # check against test set\n",
        "    print(\"Test accuracy \", session.run(acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1YZPl1QhnYl"
      },
      "source": [
        "# plot the train errors\n",
        "plt.plot(train_errors)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q-Jrlq7hnYn"
      },
      "source": [
        "<hr />\n",
        "\n",
        "<h2>Abgabe</h2>\n",
        "\n",
        "Bevor sie das Notebook in Moodle hochladen entfernen sie bitte über \"Kernel\" -> \"Restart and Clear Output\" sämtlichen von Python erstellten Inhalt und speichern anschließend das Notebook \"File\" -> \"Save and Checkpoint\" erneut ab. Sorgen sie bitte außerdem dafür das im Dateinamen ihr Vor- und Nachname steht, ich empfehle folgende Namensgebung: \"06_Tensorflow_ConvNet_MNIST_VORNAME_NACHNAME.ipynb\""
      ]
    }
  ]
}