{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LoK96yOQ5E2w"
   },
   "source": [
    "<h1>Logistische Regression mit Tensorflow anhand der MNIST Ziffern</h1>\n",
    "\n",
    "<p>Bisher haben wir mit numpy die ML Modelle und deren Ableitungen manuell programmmiert. Gute Machine Learning Frameworks können uns einiges an Arbeit abnehmen. Dank automatischer Differenzierung müssen wir uns zum Beispiel keine Gedanken mehr machen, wie die Ableitung einer Funktion aussieht. Es reicht aus, wenn wir den sogenannten Vorwärtspass bis hin zur Fehlerfunktion hinschreiben und den Rest dem Framework überlassen. In den nächsten Übungen wollen wir mit der Bibliothek Tensorflow arbeiten, welche sowohl auf der CPU als auch auf der GPU läuft. Es wird empfohlen, das Framework mit Grafikkartenunterstützung zu installieren. Voraussetzung ist eine NVIDIA-Grafikkarte, die im Labor vorzufinden ist.</p>\n",
    "\n",
    "<p>Dieses Jupyter Notebook steht wieder zum <a href=\"05_Tensorflow_Regression_MNIST_Vorlage.ipynb\" target=\"_blank\">Download</a> bereit.</p>\n",
    "\n",
    "<hr />\n",
    "<h2>Vorbereitung</h2>\n",
    "\n",
    "<p>Die bisher verwendete numpy Bibliothek arbeitet <strong>imperative </strong>und f&uuml;hrt jede geschrieben Zeile sofort aus. Niedergeschriebene Formeln werden sofort berechnet und die Ergebnisse stehen im Anschluss zur Verf&uuml;gung. <strong>Deklarative </strong>Programmierung hingegen, beschreibt nur was berechnet werden soll, aber nicht wie und wann. Python selbst ist zwar imperativ, aber der Computation Graph von Tensorflow arbeitet deklarativ. Wir unterscheiden deshalb zwischen zwei Phasen bei der Programmierung mit Tensorflow. Zun&auml;chst erstellen wir den Graphen, was f&uuml;r uns aussieht wie eine imperative Schreibweise ist in Wirklichkeit eine deklarative. Im zweiten Schritt erstellen wir eine Session, die den Graphen ausf&uuml;hrt und uns dessen Ergebnis ausgibt.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==1.15.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LElwYUQp5HHX"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow-gpu==1.15.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-0f2ne-s6e77"
   },
   "outputs": [],
   "source": [
    "!pip install deep-teaching-commons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tLsvbC-C5E3M"
   },
   "source": [
    "<hr />\n",
    "\n",
    "<h2>MNIST mit Logistischer Regression in Tensorflow</h2>\n",
    "\n",
    "<p>Implementieren Sie die logistische Regression in Tensorflow, die die Ziffern der MNIST Bilder vorhersagen kann. Die Aufgabe ist identisch zu &Uuml;bung 2. Zunächst die Daten normalisieren und in Trainungs- und Testdaten einteilen.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "id": "xVYHLkQF5E3N"
   },
   "outputs": [],
   "source": [
    "from shutil import copyfileobj\n",
    "from sklearn.datasets.base import get_data_home\n",
    "from deep_teaching_commons.data.fundamentals import mnist\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import os\n",
    "import urllib\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iQNAuA796ouE"
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = mnist.Mnist().get_all_data(normalized=True)\n",
    "\n",
    "# only shuffle train dataset\n",
    "random_state = check_random_state(0)\n",
    "permutation = random_state.permutation(X_train.shape[0])\n",
    "X_train = X_train[permutation]\n",
    "y_train = y_train[permutation]\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RuIbnzmt5E3R"
   },
   "source": [
    "Die y-Daten One-Hot kodieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6awq9XaJ5E3S"
   },
   "outputs": [],
   "source": [
    "enc = OneHotEncoder()\n",
    "\n",
    "y_train_hot = enc.fit_transform(np.expand_dims(y_train, axis=1)).toarray()\n",
    "y_test_hot = enc.fit_transform(np.expand_dims(y_test, axis=1)).toarray()\n",
    "print(y_train_hot.shape, y_test_hot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NsmPtnNA5E3W"
   },
   "source": [
    "<hr />\n",
    "<h2>Training</h2>\n",
    "\n",
    "<p>Überführen Sie die logistischen Regression von Übung 3 in eine Tensorflow-Version. Verwenden Sie Softmax als Aktivierungsfunktion und die Cross-Entropy als Fehlerfunktion. Notieren Sie sich den Trainingsfehler anhand der Trainingsdaten und die Vorhersagegenautigkeit mit Hilfe der Testdaten. Plotten Sie den Fehler und die Genauigkeit, um zu überprüfen, ob Ihr Model funktioniert. Es werden Genauigkeiten von ungefähr 92% erwartet.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kiiRnfJM8O-f"
   },
   "outputs": [],
   "source": [
    "input_size = X_train.shape[1]\n",
    "num_classes = 10\n",
    "\n",
    "iterations = 50\n",
    "learning_rate = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H9OicRHB5E3W"
   },
   "outputs": [],
   "source": [
    "# input X: 28x28 grayscale images, the first dimension (None) will index the images in the mini-batch\n",
    "X_placeholder = tf.placeholder(tf.float32, [None, input_size])\n",
    "# correct answers will go here\n",
    "Y_placeholder = tf.placeholder(tf.float32, [None, num_classes])\n",
    "\n",
    "logits = tf.layers.Dense(10)(X_placeholder)\n",
    "loss = tf.losses.softmax_cross_entropy(Y_placeholder, logits)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "opt_op = optimizer.minimize(loss)\n",
    "\n",
    "softmax = tf.nn.softmax(logits)\n",
    "prediction = tf.argmax(softmax, 1)\n",
    "labels = tf.argmax(Y_placeholder, 1)\n",
    "equality = tf.equal(prediction, labels)\n",
    "accuracy = tf.reduce_mean(tf.cast(equality, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ahP9jHM5FiCZ"
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "test_losses = []\n",
    "\n",
    "accuracies = []\n",
    "test_accuracies = []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(iterations):\n",
    "        # calculating loss value\n",
    "        l, acc, _ = sess.run([loss, accuracy, opt_op], feed_dict={X_placeholder: X_train, \n",
    "                                                                  Y_placeholder: y_train_hot})\n",
    "        losses.append(l)\n",
    "        accuracies.append(acc)\n",
    "        # trainings step\n",
    "        t_l, t_acc = sess.run([loss, accuracy], feed_dict={X_placeholder: X_test, \n",
    "                                                          Y_placeholder: y_test_hot})\n",
    "        print(l, t_l)\n",
    "        print(acc, t_acc)\n",
    "        print(\"-\" * 10)\n",
    "\n",
    "        test_losses.append(t_l)\n",
    "        test_accuracies.append(t_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XNutEOPq5E3b"
   },
   "outputs": [],
   "source": [
    "# TODO Plot the error and accuracy curves\n",
    "# TODO Plot the training and test error for each trainings iteration\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(np.arange(0, iterations), losses, \"b\", label=\"train error\")\n",
    "plt.plot(np.arange(0, iterations), test_losses, \"r\", label=\"test error\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ypiQzX_1_bsl"
   },
   "outputs": [],
   "source": [
    "# TODO Plot the training and test error for each trainings iteration\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(np.arange(0, iterations), accuracies, \"b\", label=\"train acc\")\n",
    "plt.plot(np.arange(0, iterations), test_accuracies, \"r\", label=\"test acc\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F7PUMTZz5E3d"
   },
   "source": [
    "<hr />\n",
    "<h2>Neural Network</h2>\n",
    "\n",
    "<p>Laut Yann LeCun's Datenbank <a href=\"http://yann.lecun.com/exdb/mnist/\" target=\"_blank\">http://yann.lecun.com/exdb/mnist/</a> sind Neuronale Netzwerke mit 2-3 Schichten besser als einfache Regressionsmodelle. Erweitern Sie Ihr Model zu einem Neuronalen Netzwerk mit mindestens einem Hidden-Layer. Verwenden Sie für diese Schichten die Sigmoid- oder ReLu-Aktivierungsfunktion. Geben Sie wieder den Trainingsfehler und die Testgenauigkeit in einem Diagramm aus. Zu erwarten sind Genauigkeiten von ca. 95%.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HalA_0Tn5E3e"
   },
   "outputs": [],
   "source": [
    "learning_rate = 1\n",
    "iterations = 100\n",
    "\n",
    "# input X: 28x28 grayscale images, the first dimension (None) will index the images in the mini-batch\n",
    "X_placeholder = tf.placeholder(tf.float32, [None, input_size])\n",
    "# correct answers will go here\n",
    "Y_placeholder = tf.placeholder(tf.float32, [None, num_classes])\n",
    "\n",
    "hidden = tf.layers.Dense(200 , activation=\"relu\")(X_placeholder)\n",
    "logits = tf.layers.Dense(10)(hidden)\n",
    "loss = tf.losses.softmax_cross_entropy(Y_placeholder, logits)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "opt_op = optimizer.minimize(loss)\n",
    "\n",
    "softmax = tf.nn.softmax(logits)\n",
    "prediction = tf.argmax(softmax, 1)\n",
    "labels = tf.argmax(Y_placeholder, 1)\n",
    "equality = tf.equal(prediction, labels)\n",
    "accuracy = tf.reduce_mean(tf.cast(equality, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p4eRUwgrDK8z"
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "test_losses = []\n",
    "\n",
    "accuracies = []\n",
    "test_accuracies = []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(iterations):\n",
    "        # calculating loss value\n",
    "        l, acc, _ = sess.run([loss, accuracy, opt_op], feed_dict={X_placeholder: X_train, \n",
    "                                                                  Y_placeholder: y_train_hot})\n",
    "        losses.append(l)\n",
    "        accuracies.append(acc)\n",
    "        # trainings step\n",
    "        t_l, t_acc = sess.run([loss, accuracy], feed_dict={X_placeholder: X_test, \n",
    "                                                           Y_placeholder: y_test_hot})\n",
    "\n",
    "        test_losses.append(t_l)\n",
    "        test_accuracies.append(t_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G9zHFE3uDPVo"
   },
   "outputs": [],
   "source": [
    "# TODO Plot the error and accuracy curves\n",
    "# TODO Plot the training and test error for each trainings iteration\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(np.arange(0, iterations), losses, \"b\", label=\"train error\")\n",
    "plt.plot(np.arange(0, iterations), test_losses, \"r\", label=\"test error\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wRczs6YLDQ4a"
   },
   "outputs": [],
   "source": [
    "# TODO Plot the training and test error for each trainings iteration\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(np.arange(0, iterations), accuracies, \"b\", label=\"train acc\")\n",
    "plt.plot(np.arange(0, iterations), test_accuracies, \"r\", label=\"test acc\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VXmvxPO05E3g"
   },
   "source": [
    "<hr />\n",
    "\n",
    "<h2>Abgabe</h2>\n",
    "\n",
    "Bevor sie das Notebook in Moodle hochladen entfernen sie bitte über \"Kernel\" -> \"Restart and Clear Output\" sämtlichen von Python erstellten Inhalt und speichern anschließend das Notebook \"File\" -> \"Save and Checkpoint\" erneut ab. Sorgen sie bitte außerdem dafür das im Dateinamen ihr Vor- und Nachname steht, ich empfehle folgende Namensgebung: \"05_Tensorflow_Regression_MNIST_VORNAME_NACHNAME.ipynb\""
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "05_Tensorflow_Regression_MNIST_Steven-Mi.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
